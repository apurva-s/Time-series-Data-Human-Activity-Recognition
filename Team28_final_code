{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Team28_final_code","provenance":[{"file_id":"1Ar9ZO31MNkdOty9TrN41CrgFLPEH6mck","timestamp":1618381329127}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iqT2NZkWDCEe"},"source":["**Import Libraries and Mount Google Drive**\n","\n","The keras and tenserflow libraries were used to create our 1D CNN model.  Sklearn libraries were used to normalize the datasets.  There may be ignorable warnings when importing the libraries."]},{"cell_type":"code","metadata":{"id":"RVqE_ysiJJod"},"source":["from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as stats\n","\n","import keras\n","from keras.datasets import cifar10\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization, Dropout,Conv1D,MaxPooling1D,LSTM,Embedding,GaussianNoise,UpSampling1D\n","\n","from tensorflow.keras import regularizers\n","\n","from keras.optimizers import SGD, Adam, RMSprop\n","import matplotlib.pyplot as plt\n","\n","from sklearn.utils import class_weight\n","import math\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkR4qbX9_Sr_"},"source":["from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3msAO8ZatrQQ"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTh8kZJ3Fn5g"},"source":["**Function to Plot Graph**"]},{"cell_type":"code","metadata":{"id":"p5yt3hEYDUUI"},"source":["# Defining a function for plotting training and validation learning curves\n","def plot_history(history):\n","\t  # plot loss\n","    plt.title('Loss')\n","    plt.plot(history.history['loss'], color='blue', label='train')\n","    plt.plot(history.history['val_loss'], color='red', label='test')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'])\n","    plt.show()\n","    \n","    # plot accuracy\n","    plt.title('Accuracy')\n","    plt.plot(history.history['accuracy'], color='blue', label='train')\n","    plt.plot(history.history['val_accuracy'], color='red', label='test')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'])\n","    plt.show()\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jABJwAkat607"},"source":["**Functions used to format individual rows into seconds of activity**"]},{"cell_type":"markdown","metadata":{"id":"SaKRaAy7FwCu"},"source":["This function is used to split the training data into n_step samples.  There is no overlap between samples.\n","\n","> Indented block\n","\n"]},{"cell_type":"code","metadata":{"id":"PXXx65sYnLu6"},"source":["# split a univariate sequence into samples\n","def split_sequence(xsequence,ysequence,n_steps):\n","\tX, y = list(), list()\n","\ti = 0\n","\twhile i < len(xsequence):\n","\t\t# find the end of this pattern\n","\t\tend_ix = i + n_steps\n","\t\t# check if we are beyond the sequence\n","\t\tif end_ix > len(xsequence)-1:\n","\t\t\tbreak\n","\t\t# gather input and output parts of the pattern\n","\t\tseq_x = xsequence[i:end_ix]\n","\t\tseq_y = np.bincount(ysequence[i:end_ix]).argmax()\n","\t\ti = i+n_steps\n","\t\tX.append(seq_x)\n","\t\ty.append(seq_y)\n","\treturn np.array(X), np.array(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zzpfI03jF5H0"},"source":["This function is used to split the test data into n_step samples.  Again there is no overlap.  Only the X data is returned since this is for a prediction set."]},{"cell_type":"code","metadata":{"id":"I0tDXpVixc3Y"},"source":["def split_predictions_sequence(xsequence,n_steps):\n","\tX = list()\n","\ti = 0\n","\twhile i < len(xsequence):\n","\t\t# find the end of this pattern\n","\t\tend_ix = i + n_steps\n","\t\t# check if we are beyond the sequence\n","\t\tif end_ix > len(xsequence)-1:\n","\t\t\tbreak\n","\t\t# gather input and output parts of the pattern\n","\t\tseq_x = xsequence[i:end_ix]\n","\t\ti = i+n_steps\n","\t\tX.append(seq_x)\n","\treturn np.array(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aMQaIggS74AA"},"source":["**Import Data From CSV**\n","\n","Upload the .csv data to your google drive, adjust the file_path variable if needed. The input values are too large and need to be standardized."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"leWB_vUnLTpy","executionInfo":{"status":"ok","timestamp":1618970757634,"user_tz":240,"elapsed":217,"user":{"displayName":"Nicholas Viado","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizan_QTqFniePdy4b4HFoWjl6kdnbONLAAnM5x=s64","userId":"17804541991072215371"}},"outputId":"ef9a47d3-7ae5-4bdd-bc2f-b9f7aa24ac08"},"source":["file_path = '/content/drive/My Drive/ProjectC2Final/final.csv' "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/ProjectC2Final/final.csv'"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"YIDG9uoMt-Tx","colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"status":"ok","timestamp":1618970022038,"user_tz":240,"elapsed":2104,"user":{"displayName":"Nicholas Viado","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizan_QTqFniePdy4b4HFoWjl6kdnbONLAAnM5x=s64","userId":"17804541991072215371"}},"outputId":"b21627f9-f5be-4709-c4e0-2ec499967872"},"source":["input = pd.read_csv(file_path,usecols=[\"xa\",\t\"ya\",\t\"za\",\t\"xg\",\t\"yg\",\t\"zg\",\t\"time\",\t\"subject_name\",\t\"Label\"])\n","input"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>xa</th>\n","      <th>ya</th>\n","      <th>za</th>\n","      <th>xg</th>\n","      <th>yg</th>\n","      <th>zg</th>\n","      <th>time</th>\n","      <th>subject_name</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.186920</td>\n","      <td>8.344455</td>\n","      <td>2.908057</td>\n","      <td>0.005771</td>\n","      <td>-0.004480</td>\n","      <td>-0.003345</td>\n","      <td>0.02</td>\n","      <td>subject_001__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.544637</td>\n","      <td>8.408659</td>\n","      <td>2.890000</td>\n","      <td>0.007967</td>\n","      <td>0.022412</td>\n","      <td>0.001159</td>\n","      <td>0.05</td>\n","      <td>subject_001__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.849308</td>\n","      <td>8.411614</td>\n","      <td>2.900692</td>\n","      <td>0.027778</td>\n","      <td>-0.010670</td>\n","      <td>-0.014223</td>\n","      <td>0.07</td>\n","      <td>subject_001__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.509190</td>\n","      <td>8.118649</td>\n","      <td>2.847298</td>\n","      <td>0.021577</td>\n","      <td>-0.045498</td>\n","      <td>-0.021111</td>\n","      <td>0.10</td>\n","      <td>subject_001__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.226515</td>\n","      <td>8.273807</td>\n","      <td>2.851742</td>\n","      <td>0.012534</td>\n","      <td>0.000445</td>\n","      <td>-0.016830</td>\n","      <td>0.12</td>\n","      <td>subject_001__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1341612</th>\n","      <td>2.072244</td>\n","      <td>8.908878</td>\n","      <td>-3.500000</td>\n","      <td>0.001351</td>\n","      <td>0.001191</td>\n","      <td>0.001031</td>\n","      <td>1203.32</td>\n","      <td>subject_008__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1341613</th>\n","      <td>2.085123</td>\n","      <td>8.915123</td>\n","      <td>-3.520000</td>\n","      <td>0.001918</td>\n","      <td>-0.001147</td>\n","      <td>0.000000</td>\n","      <td>1203.34</td>\n","      <td>subject_008__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1341614</th>\n","      <td>2.083774</td>\n","      <td>8.910000</td>\n","      <td>-3.538981</td>\n","      <td>-0.002015</td>\n","      <td>-0.004099</td>\n","      <td>0.001042</td>\n","      <td>1203.37</td>\n","      <td>subject_008__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1341615</th>\n","      <td>2.111447</td>\n","      <td>8.908553</td>\n","      <td>-3.535724</td>\n","      <td>0.000183</td>\n","      <td>-0.001673</td>\n","      <td>0.001856</td>\n","      <td>1203.40</td>\n","      <td>subject_008__1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1341616</th>\n","      <td>2.087730</td>\n","      <td>8.900000</td>\n","      <td>-3.518865</td>\n","      <td>0.000052</td>\n","      <td>-0.001267</td>\n","      <td>0.000000</td>\n","      <td>1203.42</td>\n","      <td>subject_008__1</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1341617 rows × 9 columns</p>\n","</div>"],"text/plain":["               xa        ya        za  ...     time    subject_name  Label\n","0        4.186920  8.344455  2.908057  ...     0.02  subject_001__1    0.0\n","1        4.544637  8.408659  2.890000  ...     0.05  subject_001__1    0.0\n","2        4.849308  8.411614  2.900692  ...     0.07  subject_001__1    0.0\n","3        4.509190  8.118649  2.847298  ...     0.10  subject_001__1    0.0\n","4        4.226515  8.273807  2.851742  ...     0.12  subject_001__1    0.0\n","...           ...       ...       ...  ...      ...             ...    ...\n","1341612  2.072244  8.908878 -3.500000  ...  1203.32  subject_008__1    0.0\n","1341613  2.085123  8.915123 -3.520000  ...  1203.34  subject_008__1    0.0\n","1341614  2.083774  8.910000 -3.538981  ...  1203.37  subject_008__1    0.0\n","1341615  2.111447  8.908553 -3.535724  ...  1203.40  subject_008__1    0.0\n","1341616  2.087730  8.900000 -3.518865  ...  1203.42  subject_008__1    0.0\n","\n","[1341617 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"mCeq82h4GCpR"},"source":["Encode output with labels and standardize the X data.  "]},{"cell_type":"code","metadata":{"id":"RZMg5YwCvjFt","colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"status":"ok","timestamp":1618970068285,"user_tz":240,"elapsed":654,"user":{"displayName":"Nicholas Viado","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizan_QTqFniePdy4b4HFoWjl6kdnbONLAAnM5x=s64","userId":"17804541991072215371"}},"outputId":"31b98668-5bc2-4257-cfc8-f75cb92482eb"},"source":["label = LabelEncoder()\n","input['Label']=label.fit_transform(input['Label'])\n","scaler = StandardScaler()\n","newX=input[['xa','ya','za','xg','yg','zg']]\n","Y=input['Label'] \n","newX = scaler.fit_transform(newX)\n","scaled_X = pd.DataFrame(data = newX, columns = ['Ax','Ay','Az','Gx','Gy','Gz'])\n","scaled_X['label'] = Y.values\n","scaled_X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Ax</th>\n","      <th>Ay</th>\n","      <th>Az</th>\n","      <th>Gx</th>\n","      <th>Gy</th>\n","      <th>Gz</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.620543</td>\n","      <td>0.045290</td>\n","      <td>-0.567099</td>\n","      <td>0.000351</td>\n","      <td>-0.018272</td>\n","      <td>0.083457</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.682938</td>\n","      <td>0.054657</td>\n","      <td>-0.570337</td>\n","      <td>0.001033</td>\n","      <td>-0.001082</td>\n","      <td>0.089309</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.736081</td>\n","      <td>0.055088</td>\n","      <td>-0.568419</td>\n","      <td>0.007191</td>\n","      <td>-0.022229</td>\n","      <td>0.069323</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.676755</td>\n","      <td>0.012346</td>\n","      <td>-0.577995</td>\n","      <td>0.005263</td>\n","      <td>-0.044493</td>\n","      <td>0.060374</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.627449</td>\n","      <td>0.034983</td>\n","      <td>-0.577198</td>\n","      <td>0.002453</td>\n","      <td>-0.015124</td>\n","      <td>0.065937</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1341612</th>\n","      <td>0.251684</td>\n","      <td>0.127635</td>\n","      <td>-1.716335</td>\n","      <td>-0.001023</td>\n","      <td>-0.014647</td>\n","      <td>0.089143</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1341613</th>\n","      <td>0.253931</td>\n","      <td>0.128546</td>\n","      <td>-1.719921</td>\n","      <td>-0.000847</td>\n","      <td>-0.016142</td>\n","      <td>0.087803</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1341614</th>\n","      <td>0.253695</td>\n","      <td>0.127799</td>\n","      <td>-1.723325</td>\n","      <td>-0.002069</td>\n","      <td>-0.018029</td>\n","      <td>0.089157</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1341615</th>\n","      <td>0.258522</td>\n","      <td>0.127588</td>\n","      <td>-1.722741</td>\n","      <td>-0.001386</td>\n","      <td>-0.016478</td>\n","      <td>0.090215</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1341616</th>\n","      <td>0.254385</td>\n","      <td>0.126340</td>\n","      <td>-1.719718</td>\n","      <td>-0.001427</td>\n","      <td>-0.016218</td>\n","      <td>0.087803</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1341617 rows × 7 columns</p>\n","</div>"],"text/plain":["               Ax        Ay        Az        Gx        Gy        Gz  label\n","0        0.620543  0.045290 -0.567099  0.000351 -0.018272  0.083457      0\n","1        0.682938  0.054657 -0.570337  0.001033 -0.001082  0.089309      0\n","2        0.736081  0.055088 -0.568419  0.007191 -0.022229  0.069323      0\n","3        0.676755  0.012346 -0.577995  0.005263 -0.044493  0.060374      0\n","4        0.627449  0.034983 -0.577198  0.002453 -0.015124  0.065937      0\n","...           ...       ...       ...       ...       ...       ...    ...\n","1341612  0.251684  0.127635 -1.716335 -0.001023 -0.014647  0.089143      0\n","1341613  0.253931  0.128546 -1.719921 -0.000847 -0.016142  0.087803      0\n","1341614  0.253695  0.127799 -1.723325 -0.002069 -0.018029  0.089157      0\n","1341615  0.258522  0.127588 -1.722741 -0.001386 -0.016478  0.090215      0\n","1341616  0.254385  0.126340 -1.719718 -0.001427 -0.016218  0.087803      0\n","\n","[1341617 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"uF9ayGwO8KZe"},"source":["**Sampling Rate**\n","\n","Recording rate is the frequency at which the input was recorded, in this example, it was recorded at 40hz, therefore it is set to 40.  Sample_length is the amount of time you want to count as an activity.  The formula is RecordingRate * *Number of seconds*.  So if you wanted a 2 second sample, it would be set to RecordingRate * 2.  "]},{"cell_type":"code","metadata":{"id":"DV5u4o6LxKK-"},"source":["RecordingRate=40\n","Sample_length = RecordingRate\n","n_features = 6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3SwscoVnlnI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618970105759,"user_tz":240,"elapsed":610,"user":{"displayName":"Nicholas Viado","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizan_QTqFniePdy4b4HFoWjl6kdnbONLAAnM5x=s64","userId":"17804541991072215371"}},"outputId":"240b682f-c15a-4681-b7d2-7694b07e4291"},"source":["X = scaled_X[['Ax','Ay','Az','Gx','Gy','Gz']]\n","Y = scaled_X['label']\n","Y.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    1006897\n","3     206436\n","2      73068\n","1      55216\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"wFZBnjFpxNBq"},"source":["Xarray, Yarray=split_sequence(X.to_numpy(),Y.to_numpy(),Sample_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xbW4tBkODGlU"},"source":["**Cross Validation Test (Optional)**\n","\n","Test Cross Validation - Dont run this if not needed"]},{"cell_type":"code","metadata":{"id":"GNP4XlXYDK4V"},"source":["kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n","cvscores = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z1VwypiIEzWd"},"source":["#Apurva\n","for train,test in kfold.split(Xarray,Yarray):\n","  # define model\n","  model = Sequential()\n","  model.add(LSTM(units=64, input_shape=[(Xarray[1], Yarray[2])]))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(64, activation='relu'))\n","  model.add(Dense(6,activation='softmax'))\n","  model.compile(optimizer=Adam(learning_rate=0.00001),loss='categorical_crossentropy',metrics = ['accuracy'])\n","  model.fit(Xarray[train], Yarray[train], epochs=15, batch_size=10, shuffle = True, verbose=1)\n","  # evaluate the model\n","  scores = model.evaluate(Xarray[test], Yarray[test], verbose=0)\n","  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","  cvscores.append(scores[1] * 100)\n","print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4Mxt3wxDRtE"},"source":["for train,test in kfold.split(Xarray,Yarray):\n","  # define model\n","  model = Sequential()\n","  model.add(Conv1D(64,3,activation='relu',input_shape=(Sample_length, n_features),padding='same'))\n","  model.add(Dropout(0.5))\n","  model.add(MaxPooling1D(2))\n","\n","  model.add(Conv1D(128,3,activation='relu',padding='same'))\n","  model.add(Dropout(0.5))\n","  model.add(MaxPooling1D(2))\n","\n","  model.add(Conv1D(128,3,activation='relu',padding='same'))\n","  model.add(Dropout(0.5))\n","  model.add(UpSampling1D(2))\n","\n","  model.add(Conv1D(64,3,activation='relu',padding='same'))\n","  model.add(Dropout(0.5))\n","  model.add(UpSampling1D(2))\n","\n","  model.add(Flatten())\n","  model.add(Dense(4,activation='softmax'))\n","  model.compile(optimizer=Adam(learning_rate=0.0001),loss='sparse_categorical_crossentropy',metrics = ['accuracy'])\n","  model.fit(Xarray[train], Yarray[train], epochs=15, batch_size=10, verbose=1)\n","  # evaluate the model\n","  scores = model.evaluate(Xarray[test], Yarray[test], verbose=0)\n","  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","  cvscores.append(scores[1] * 100)\n","print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TLrmp04HBp0"},"source":["print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j04ufCKHDLaa"},"source":["**Normal Validation/Training**"]},{"cell_type":"markdown","metadata":{"id":"Eg0Mz9WrGU2l"},"source":["Reshape the X and Y Data"]},{"cell_type":"code","metadata":{"id":"KvUZQNZ1oEcW"},"source":["Xarray = Xarray.reshape((Xarray.shape[0]),Xarray.shape[1],n_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SwONN15Noj6v"},"source":["Xarray.shape,Yarray.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9Zy6NGL4Tgt"},"source":["Counter(Yarray)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7aCtPRJ8j5y"},"source":["**Test and Train Data Split**"]},{"cell_type":"markdown","metadata":{"id":"XC6bGzkJ8nqb"},"source":["Right now the data is split 20% (test_size) variable."]},{"cell_type":"code","metadata":{"id":"MA0HYGfh0qoy"},"source":["X_train, X_test,Y_Train,Y_Test = train_test_split(Xarray,Yarray, test_size = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IucoxSsj4WB3"},"source":["Counter(Y_Train),Counter(Y_Test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSqC5l7s858N"},"source":["**1D CNN Model**\n","\n","We tested 2 different 1D CNNs.  The multi-layered CNN performed the best, but took significantly longer to run.\n","\n","This is the basic 1D CNN, it achieves a 0.62 F1 score with around 20 epochs"]},{"cell_type":"code","metadata":{"id":"_eedWGfdoKnX"},"source":["# define model\n","model = Sequential()\n","model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(Sample_length, n_features),padding='same'))\n","model.add(Dropout(0.5))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(4,activation='softmax'))\n","model.compile(optimizer=Adam(learning_rate=0.0001),loss='sparse_categorical_crossentropy',metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFFtCJKnHKMI"},"source":["This is a multi layered 1D CNN, it achieves around a 0.87 F1 score with around 45 epochs."]},{"cell_type":"code","metadata":{"id":"Z0zgWh48JeLg"},"source":["model = Sequential()\n","\n","model.add(Conv1D(256,3,activation='relu',input_shape=(Sample_length, n_features),padding='same'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","\n","model.add(Conv1D(512,3,activation='relu',padding='same'))\n","model.add(Dropout(0.3))\n","model.add(MaxPooling1D(2))\n","\n","model.add(Conv1D(512,3,activation='relu',padding='same'))\n","model.add(Dropout(0.3))\n","model.add(UpSampling1D(2))\n","\n","model.add(Conv1D(256,3,activation='relu',padding='same'))\n","model.add(Dropout(0.3))\n","model.add(UpSampling1D(2))\n","\n","model.add(Flatten())\n","model.add(Dense(4,activation='softmax'))\n","model.compile(optimizer=Adam(learning_rate=0.0001),loss='sparse_categorical_crossentropy',metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXAPig0o3xqY"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUfZ2Vnm_ZIV"},"source":["**Automatic Weights**"]},{"cell_type":"markdown","metadata":{"id":"aTQBGRKxHcHW"},"source":["This is used to help with the data imbalance.  Use this to automatically calculate what weights to use.  Either use automatic or manual weights, do not use both)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lG6ssx3LyfJ_","executionInfo":{"status":"ok","timestamp":1618970385414,"user_tz":240,"elapsed":323,"user":{"displayName":"Nicholas Viado","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizan_QTqFniePdy4b4HFoWjl6kdnbONLAAnM5x=s64","userId":"17804541991072215371"}},"outputId":"df586a4d-0182-406e-b100-352aaa37fa6c"},"source":["weights = dict(enumerate(class_weight.compute_class_weight('balanced',np.unique(Y_Train),Y_Train).flatten(),0))\n","weights"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 0.33272159119091316,\n"," 1: 6.043243243243243,\n"," 2: 4.619834710743802,\n"," 3: 1.6325139936724264}"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"3dIwJi0eHZCR"},"source":["**Train the Model**"]},{"cell_type":"code","metadata":{"id":"CKQ1MsV2oOOw"},"source":["history = model.fit(X_train,Y_Train,epochs=30,validation_data=(X_test,Y_Test),verbose=1,shuffle=True,class_weight=weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xo1X6vaf1Ho6"},"source":["plot_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MlxuRjLAH3ko"},"source":["**Run the predictions for subjects 9 to 12**\n","\n","Before running the prediction create the samples and standardize the data. Specify the path to the directory containing the prediction datasets.  You do not have to specify each individual csv, only the directory."]},{"cell_type":"code","metadata":{"id":"74XDPhvtxXgQ"},"source":["# User must specify this directory\n","PredictionDirectory = '/content/drive/My Drive/ProjectC2Final/TestData/'\n","ypredictions = []\n","for i in range(9,13):\n","  if i == 9:\n","    PredictionCSV = PredictionDirectory + '/subject_009_01__x.csv'\n","    test_input = pd.read_csv(PredictionCSV,names=[\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"])\n","  else:\n","    PredictionCSV = PredictionDirectory + 'subject_0{0}_01__x.csv'.format(i)\n","    test_input = pd.read_csv(PredictionCSV,names=[\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"])\n","  newX=test_input[['Ax','Ay','Az','Gx','Gy','Gz']]\n","  newX = scaler.transform(newX)\n","  scaled_X = pd.DataFrame(data = newX, columns = ['Ax','Ay','Az','Gx','Gy','Gz'])\n","  X = scaled_X[['Ax','Ay','Az','Gx','Gy','Gz']]\n","  X_data = split_predictions_sequence(X.to_numpy(),Sample_length)\n","  X_data = X_data.reshape((X_data.shape[0]),X_data.shape[1],n_features)\n","  \n","  ypredictions.append(model.predict(X_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDSq740IIKwL"},"source":["**Transform Y Data**\n","\n","The Y data is not in the correct form, instead of having 1 output, there are 4 outputs which represent the probability of what class the output represents.  For example Y[0] might look like [0.1,0.2,0.6,0.1].  The third index has the highest probability so the output is set to whatever that index represents."]},{"cell_type":"code","metadata":{"id":"peu3rbPSyVtZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618970942131,"user_tz":240,"elapsed":4800,"user":{"displayName":"Nicholas Viado","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizan_QTqFniePdy4b4HFoWjl6kdnbONLAAnM5x=s64","userId":"17804541991072215371"}},"outputId":"2b5635c8-ee74-4852-c770-25a1ed5e2b7b"},"source":["class_names = ['0','1','2','3']\n","argmax_predictions=[]\n","for j in range (0,4):\n","  print(\"Subject {} Predictions Complete\".format(j+9))\n","  #print(\"---------------------------------------------------\")\n","  argmax_df = pd.DataFrame({'label':[]})\n","  for i in range (0,len(ypredictions[j])):\n","    #print(class_names[np.argmax(ypredictions[j][i])])\n","    add_df = pd.DataFrame({'label':[np.argmax(ypredictions[j][i])]})\n","    argmax_df = argmax_df.append(add_df)\n","  argmax_df['label'] = argmax_df['label'].astype('int')\n","  argmax_predictions.append(argmax_df)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Subject 9 Predictions Complete\n","Subject 10 Predictions Complete\n","Subject 11 Predictions Complete\n","Subject 12 Predictions Complete\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3tlZhOrgItFU"},"source":["**Count the number of outputs for each classification in the prediction set**\n","\n","This can give you a general idea of whether or not your model is biasing one or two classes"]},{"cell_type":"code","metadata":{"id":"gfxMtnFlx97m"},"source":["for j in range(0,4):\n","  print(\"Subject {} Labels\".format(j+9))\n","  print(argmax_predictions[j].value_counts())\n","  print(\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iM8fjUuSIzy6"},"source":["**Create the Prediction CSVs**\n","\n","Because we divide the inputs into 1 second samples (composed of 40 rows of inputs), there will be less outputs than expected.  The predictions that are created are interpolated to match the expected number of outputs.  This is done by dividing the number of expected outputs by the number of predictions.  For example, if our model produces 100 predictions, but we need to compare those to 1000 outputs, the 100 predicitons would be duplicated by multiples of 10 (1000/100) in a for loop."]},{"cell_type":"code","metadata":{"id":"0FUwIpIOMbU3"},"source":["PredictionCSV = pd.read_csv(PredictionDirectory + '/subject_009_01__y_time.csv',names=[\"label\"])\n","output_length = len(PredictionCSV)\n","loop_length = math.floor(output_length/len(ypredictions[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36zOUQoW1cvD"},"source":["class_names = ['0','1','2','3']\n","\n","for i in range(9,13):\n","  print(i)\n","  if i == 9:\n","    test_input = pd.read_csv(PredictionDirectory + '/subject_009_01__y_time.csv',names=[\"label\"])\n","  else:\n","    test_input = pd.read_csv(PredictionDirectory + '/subject_0{}_01__y_time.csv'.format(i),names=[\"label\"])\n","\n","  output_length = len(test_input)\n","  print(\"Expected Length : \",output_length)\n","  print(\"Prediction Length : \",len(ypredictions[i-9]))\n","\n","  predictions_csv = pd.DataFrame({'Class':[]})\n","  for x in range(0,len(ypredictions[i-9])):\n","    new_df = pd.DataFrame({'Class':[class_names[np.argmax(ypredictions[i-9][x])]]})\n","    for j in range(0,loop_length):\n","      predictions_csv = predictions_csv.append(new_df)\n","  print(len(predictions_csv))\n","\n","  while len(predictions_csv) != output_length:\n","    new_df = predictions_csv.tail(1)\n","    predictions_csv = predictions_csv.append(new_df)\n","  print(len(predictions_csv))\n","  predictions_csv.to_csv('subject_0{0}_01__y_prediction.csv'.format(i),header=False,index=False)\n","  files.download('subject_0{0}_01__y_prediction.csv'.format(i))"],"execution_count":null,"outputs":[]}]}